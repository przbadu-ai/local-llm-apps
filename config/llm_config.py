# If you have a local LLM server running, set the URL here
# LOCAL_LLM_URL = "http://localhost:11434"  # Example URL for
LLM_MODEL = "llama3.1:latest"  # "gpt-oss-20b", "gpt-oss-120b", "qwen3-30b-a3b"
EMBEDDING_MODEL = "bge-m3:latest"  # "text-embedding-3-small"
EVALUATION_LLM_MODEL = "llama3.1:latest"

# Chroma db path
CHROMADB_PATH = "./chroma_db"
